{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Report for Data Challenge\n",
    "\n",
    "### **Challenge Description: eCommerce Inventory Planning for Multiple Products**\n",
    "\n",
    "Build a time series forecasting model that helps eCommerce merchants plan their monthly inventory purchasing for the year 2023 for multiple products on M5-Accuracy Compatation data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Description\n",
    "\n",
    "The M5 dataset, generously made available by Walmart, involves the unit sales of various products sold in the USA, organized in the form of grouped time series. More specifically, the dataset involves the unit sales of 3,049 products, classified in 3 product categories (Hobbies, Foods, and Household) and 7 product departments, in which the above-mentioned categories are disaggregated.  The products are sold across ten stores, located in three States (CA, TX, and WI). In this respect, the bottom-level of the hierarchy, i.e., product-store unit sales can be mapped across either product categories or geographical regions, as follows:\n",
    "\n",
    "![](images/dataset_diagram.png)\n",
    "\n",
    "image source: kaggle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Insights from EDA\n",
    "\n",
    "### 20 different items\n",
    "\n",
    "![](images/eda1.PNG)\n",
    "\n",
    "### Sales by store category\n",
    "\n",
    "![](images/eda2.PNG)\n",
    "\n",
    "\n",
    "![](images/eda3.PNG)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Feature Selection and Engineering\n",
    "- Data Features\n",
    "- Lag Features\n",
    "- Creating new fetures\n",
    "- Standard Scalar to normalize the features\n",
    "\n",
    "## Data Preprocessing\n",
    "- Merged and melted data frames\n",
    "- Removed unnecessary columns\n",
    "- Cleaned from noises\n",
    "- Created new columns\n",
    "- Splitting data into training and testing datasets\n",
    "- Normalized the features with Standard Scalar\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Model Selection\n",
    "I use the RMSE and norm_error loss evaluation metric for each model to select the highest performing model.  The model with the lowest RMSE has the best fit to the data for forecasting the m5 sales data.  The models I use were Holt-Linear (exponantial smoothing),Holt-Winter (Double Exponantial Smooothing),ARIMA (Mooving Avarage), SARIMAX, LGBM Regressor. The best performing model is LGBM Regressor with approximately 2.3 (even for 120 itarations) RMSE.  This is the model I will use for the forecasting of m5 sales data for 28 days (a month by 4 weeks which is important while forecasting data has weekly seasonality).\n",
    "\n",
    "\n",
    "*** Also in lgbm modelling notebook (model&data_pipeline.ipynb) I created data pipeline from sunmission data which can be anytime new came data. ***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relative Important Features\n",
    "\n",
    "![](images/lgbm_importances-01.PNG)\n",
    "\n",
    "The light GBM model performed fairly well with an RMSE over the folds is approximately 2.3. I can see from the relative feature importance that the top 3 features were item_id, store_id, sell_price, and week. I think item_id and sell_price makes sense to be the top features for the forecast because the forecast will depend on the items and price of the items. Week also makes sense because the forecast depends on time parameters to make a forecast, so having a time parameter in the top three relative important features is logical."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Further Investigation\n",
    "- More Feture Engineering for prophet and sarimax models.\n",
    "- Developing a Neural Network model with LSTM and comparing lastyly tuned Prophet, tuned SRIMAX, LSTM and LGBM.\n",
    "- Creating dynamic data pipline with last selected model on cloud or onprem with APIs (FastAPI or Flask API)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook File Explanations\n",
    "\n",
    "### File single shot:\n",
    "\n",
    "- Understanding data\n",
    "- Defineing and handling trends, seasonality\n",
    "- single product forecasting with sarimax and prophet\n",
    "\n",
    "### File eda-ml and stats:\n",
    "\n",
    "- well explained EDA on data\n",
    "- handling missing values and feture enginering\n",
    "- handling trendding, seasonality and noises\n",
    "- creating stat and ml forecasting models for multiple products\n",
    "- comparing models on norm-error-loss\n",
    "\n",
    "### File model and data pipeline:\n",
    "\n",
    "- developing lgbm model for best practice\n",
    "- saving model\n",
    "- creating data pipeline\n",
    "- connecting data pipeline with model to predict new datas\n",
    "- feature selecting  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources:\n",
    "\n",
    "In This work I got help from Kaggle, stackoverflow and chatGPT (detailed method explanations)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
